{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import ImageFont, ImageDraw, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포즈 감지 모델 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_video = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.7,\n",
    "                          min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포즈 검출 함수\n",
    "def detectPose(image_pose, pose, draw=False, display=False):\n",
    "    \n",
    "    original_image = image_pose.copy()\n",
    "    \n",
    "    image_in_RGB = cv2.cvtColor(image_pose, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    resultant = pose.process(image_in_RGB)\n",
    "\n",
    "    if resultant.pose_landmarks and draw:    \n",
    "\n",
    "        mp_drawing.draw_landmarks(image=original_image, landmark_list=resultant.pose_landmarks,\n",
    "                                  connections=mp_pose.POSE_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,255,255),\n",
    "                                                                               thickness=3, circle_radius=3),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(49,125,237),\n",
    "                                                                               thickness=2, circle_radius=2))\n",
    "\n",
    "    if display:\n",
    "            \n",
    "            plt.figure(figsize=[22,22])\n",
    "            plt.subplot(121);plt.imshow(image_pose[:,:,::-1]);plt.title(\"Input Image\");plt.axis('off');\n",
    "            plt.subplot(122);plt.imshow(original_image[:,:,::-1]);plt.title(\"Pose detected Image\");plt.axis('off');\n",
    "\n",
    "    else:\n",
    "        \n",
    "        return original_image, resultant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(data):\n",
    "    data = np.array(data)\n",
    "    x = data.T[0]\n",
    "    y = data.T[1]\n",
    "    z = data.T[2]\n",
    "    x_norm = (x - min(x)) / (max(x) - min(x))\n",
    "    y_norm = (y - min(y)) / (max(y) - min(y))\n",
    "    z_norm = (z - min(z)) / (max(z) - min(z))\n",
    "    \n",
    "    return (x_norm.tolist(), y_norm.tolist(), z_norm.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_vector(land):\n",
    "    link_keypoint = [(0, 1),\n",
    "        (1, 3),\n",
    "        (3,\t5),\n",
    "        (5,\t7),\n",
    "        (5,\t9),\n",
    "        (5,\t11),\n",
    "        (1, 13),\n",
    "        (13, 15),\n",
    "        (15, 17),\n",
    "        (17, 19),\n",
    "        (17, 21),\n",
    "        (0, 2),\n",
    "        (2, 4),\n",
    "        (4, 6),\n",
    "        (6, 8),\n",
    "        (6, 10),\n",
    "        (6, 12),\n",
    "        (2, 14),\n",
    "        (14, 16),\n",
    "        (16, 18),\n",
    "        (18, 20),\n",
    "        (18, 22)]    \n",
    "    \n",
    "    a = []\n",
    "    for link in link_keypoint:\n",
    "        x = land[0][link[0]] - land[0][link[1]]\n",
    "        y = land[1][link[0]] - land[1][link[1]]\n",
    "        z = land[2][link[0]] - land[2][link[1]]\n",
    "        a.append((x, y, z))\n",
    "    return np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_vector(land):\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    angle_keypoint=[\n",
    "        (0, 1, 3),\n",
    "        (1, 3, 5),\n",
    "        (3, 5, 9),\n",
    "        (1, 13, 15),\n",
    "        (13, 15, 17),\n",
    "        (15, 17, 19),\n",
    "        (15, 17, 21),\n",
    "        (0, 2, 4),\n",
    "        (2, 4, 6),\n",
    "        (4, 6, 10),\n",
    "        (2, 14, 16),\n",
    "        (14, 16, 18),\n",
    "        (16, 18, 20),\n",
    "        (16, 18, 22)]\n",
    "    \n",
    "    a = []\n",
    "    for angle in angle_keypoint:\n",
    "        x = np.array([land[0][angle[0]] - land[0][angle[1]], land[1][angle[0]] - land[1][angle[1]], land[2][angle[0]] - land[2][angle[1]]])\n",
    "        y = np.array([land[0][angle[1]] - land[0][angle[2]], land[1][angle[1]] - land[1][angle[2]], land[2][angle[1]] - land[2][angle[2]]])\n",
    "        \n",
    "        분자 = np.dot(x, y)\n",
    "        분모 = np.sqrt(x.dot(x)) * np.sqrt(x.dot(x))\n",
    "        try:\n",
    "            a.append(math.acos(분자 / 분모))\n",
    "        except:\n",
    "            a.append(0)\n",
    "    return (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_feature(link, angle):\n",
    "    산술평균_링크 = [np.mean(link.T[0]), np.mean(link.T[1]), np.mean(link.T[2])]\n",
    "    표준편차_링크 = [np.std(link.T[0]), np.std(link.T[1]), np.std(link.T[2])]\n",
    "    제곱평균_링크 = [np.mean(link.T[0]**2), np.mean(link.T[1]**2), np.mean(link.T[2]**2)]\n",
    "    \n",
    "    산술평균_앵글 = np.mean(angle)\n",
    "    표준편차_앵글 = np.std(angle)\n",
    "    제곱평균_앵글 = np.mean(angle)\n",
    "    \n",
    "    return(산술평균_링크+표준편차_링크+제곱평균_링크+[산술평균_앵글]+[표준편차_앵글]+[제곱평균_앵글])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model = joblib.load('./RandomForestFinal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(target, user):\n",
    "    target_landmarks = target.pose_world_landmarks.landmark\n",
    "    target_lm = [(i.x, i.y, i.z) for num, i in enumerate(target_landmarks) if num not in range(1, 11)]\n",
    "    target_norm = norm(target_lm)\n",
    "    target_link_vector = link_vector(target_norm)\n",
    "    target_angle_vector = angle_vector(target_norm)\n",
    "    \n",
    "    user_landmarks = user.pose_world_landmarks.landmark\n",
    "    user_lm = [(i.x, i.y, i.z) for num, i in enumerate(user_landmarks) if num not in range(1, 11)]\n",
    "    user_norm = norm(user_lm)\n",
    "    user_link_vector = link_vector(user_norm)\n",
    "    user_angle_vector = angle_vector(user_norm)\n",
    "    \n",
    "    link_diff = np.array(target_link_vector) - user_link_vector\n",
    "    angle_diff = np.array(target_angle_vector) - user_angle_vector\n",
    "    feature = pose_feature(link_diff, angle_diff); feature = np.array(feature).reshape(1, -1)\n",
    "    similarity = model.predict_proba(feature)[0][1]\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTS - Dynamite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "video_clip = VideoFileClip('./static/dance/BTS - Dynamite.mp4')\n",
    "du = video_clip.duration\n",
    "video_clip2 = VideoFileClip('./static/user_dance/Dynamite user.mp4').subclip(0.2, du+0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./target.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "audioclip = video_clip.audio\n",
    "audioclip.write_audiofile('./target.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ss_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ksw07\\Desktop\\파이널 프로젝트\\2022-Multi_PJT_3_Flask-master\\2022-Multi_PJT_3_Flask\\영상 프레임 맞추기_등급추가.ipynb 셀 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ksw07/Desktop/%ED%8C%8C%EC%9D%B4%EB%84%90%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2022-Multi_PJT_3_Flask-master/2022-Multi_PJT_3_Flask/%EC%98%81%EC%83%81%20%ED%94%84%EB%A0%88%EC%9E%84%20%EB%A7%9E%EC%B6%94%EA%B8%B0_%EB%93%B1%EA%B8%89%EC%B6%94%EA%B0%80.ipynb#ch0000043?line=0'>1</a>\u001b[0m ss_mean\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ss_mean' is not defined"
     ]
    }
   ],
   "source": [
    "ss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "# 사용자 영상만\n",
    "w = int(video_clip2.w)\n",
    "h = int(video_clip2.h)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, 30, (w, h))  # out 1/0.03 -> 1 ///\n",
    "\n",
    "ss = []\n",
    "text = ''\n",
    "\n",
    "for num, i in enumerate(np.arange(0, du, 0.03)):  # 원래는 0.03\n",
    "   \n",
    "    img = video_clip.get_frame(i)\n",
    "    img2 = video_clip2.get_frame(i)\n",
    "    \n",
    "    img_user = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if (num+1) % 20 == 0:\n",
    "        ss_mean = round(np.mean([i*100 for i in ss[num-18:] if i is not None]))\n",
    "        score = ('BAD' if ss_mean <= 30 else 'MISS' if ss_mean <= 40 else 'GOOD' if ss_mean <= 50 else 'VERY GOOD' if ss_mean <= 60 else 'PERPECT')\n",
    "        text = f'{score}'\n",
    "\n",
    "    try:\n",
    "        result = detectPose(img, pose_video)[1]\n",
    "        result2 = detectPose(img2, pose_video)[1]\n",
    "        \n",
    "        similarity = get_sim(result, result2)\n",
    "        ss.append(similarity)\n",
    "        \n",
    "\n",
    "        \n",
    "        cv2.putText(img_user,  text, (500, 50), cv2.FONT_HERSHEY_DUPLEX, 2, (199, 114, 255), 2, cv2.LINE_AA)        # BGR\n",
    "        cv2.imshow(\"target\", img_user)\n",
    "        \n",
    "    except:\n",
    "        ss.append(None)\n",
    "        \n",
    "        cv2.putText(img_user,  text, (500, 50), cv2.FONT_HERSHEY_DUPLEX, 2, (199, 114, 255), 2, cv2.LINE_AA)        # BGR\n",
    "        cv2.imshow(\"target\", img_user)\n",
    "        \n",
    "        \n",
    "    print('.', end='')\n",
    "    out.write(img_user) #프레임 쓰기\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 영상만\n",
    "w = int(video_clip2.w)\n",
    "h = int(video_clip2.h)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "out = cv2.VideoWriter('0.avi', fourcc, 1, (w, h))\n",
    "\n",
    "ss = []\n",
    "text = ''\n",
    "\n",
    "for num, i in enumerate(np.arange(0, du, 1)):\n",
    "   \n",
    "    img = video_clip.get_frame(i)\n",
    "    img2 = video_clip2.get_frame(i)\n",
    "    \n",
    "    img_user = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if (num+1) % 20 == 0:\n",
    "        ss_mean = round(np.mean([i*100 for i in ss[num-18:] if i is not None]))\n",
    "        score = ('BAD' if ss_mean <= 30 else 'MISS' if ss_mean <= 40 else 'GOOD' if ss_mean <= 50 else 'VERY GOOD' if ss_mean <= 60 else 'PERPECT')\n",
    "        text = f'{score}'\n",
    "\n",
    "    try:\n",
    "        result = detectPose(img, pose_video)[1]\n",
    "        result2 = detectPose(img2, pose_video)[1]\n",
    "        \n",
    "        similarity = get_sim(result, result2)\n",
    "        ss.append(similarity)\n",
    "\n",
    "        \n",
    "        cv2.putText(img_user,  text, (500, 50), cv2.FONT_HERSHEY_DUPLEX, 2, (199, 114, 255), 2, cv2.LINE_AA)        # BGR\n",
    "        cv2.imshow(\"target\", img_user)\n",
    "        \n",
    "    except:\n",
    "        ss.append(None)\n",
    "        \n",
    "        cv2.putText(img_user,  text, (500, 50), cv2.FONT_HERSHEY_DUPLEX, 2, (199, 114, 255), 2, cv2.LINE_AA)        # BGR\n",
    "        cv2.imshow(\"target\", img_user)\n",
    "\n",
    "    out.write(img_user)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "cannot open resource",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ksw07\\Desktop\\파이널 프로젝트\\2022-Multi_PJT_3_Flask-master\\2022-Multi_PJT_3_Flask\\영상 프레임 맞추기_등급추가.ipynb 셀 16\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ksw07/Desktop/%ED%8C%8C%EC%9D%B4%EB%84%90%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2022-Multi_PJT_3_Flask-master/2022-Multi_PJT_3_Flask/%EC%98%81%EC%83%81%20%ED%94%84%EB%A0%88%EC%9E%84%20%EB%A7%9E%EC%B6%94%EA%B8%B0_%EB%93%B1%EA%B8%89%EC%B6%94%EA%B0%80.ipynb#ch0000014?line=51'>52</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ksw07/Desktop/%ED%8C%8C%EC%9D%B4%EB%84%90%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2022-Multi_PJT_3_Flask-master/2022-Multi_PJT_3_Flask/%EC%98%81%EC%83%81%20%ED%94%84%EB%A0%88%EC%9E%84%20%EB%A7%9E%EC%B6%94%EA%B8%B0_%EB%93%B1%EA%B8%89%EC%B6%94%EA%B0%80.ipynb#ch0000014?line=53'>54</a>\u001b[0m \u001b[39m# 점수 프레임 만들기\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ksw07/Desktop/%ED%8C%8C%EC%9D%B4%EB%84%90%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2022-Multi_PJT_3_Flask-master/2022-Multi_PJT_3_Flask/%EC%98%81%EC%83%81%20%ED%94%84%EB%A0%88%EC%9E%84%20%EB%A7%9E%EC%B6%94%EA%B8%B0_%EB%93%B1%EA%B8%89%EC%B6%94%EA%B0%80.ipynb#ch0000014?line=54'>55</a>\u001b[0m font \u001b[39m=\u001b[39m ImageFont\u001b[39m.\u001b[39;49mtruetype(\u001b[39m\"\u001b[39;49m\u001b[39mC:/Users/min/AppData/Local/Microsoft/Windows/Fonts/NanumSquareOTF.otf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m40\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ksw07/Desktop/%ED%8C%8C%EC%9D%B4%EB%84%90%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2022-Multi_PJT_3_Flask-master/2022-Multi_PJT_3_Flask/%EC%98%81%EC%83%81%20%ED%94%84%EB%A0%88%EC%9E%84%20%EB%A7%9E%EC%B6%94%EA%B8%B0_%EB%93%B1%EA%B8%89%EC%B6%94%EA%B0%80.ipynb#ch0000014?line=56'>57</a>\u001b[0m img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull(ver_cv\u001b[39m.\u001b[39mshape, (\u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m), np\u001b[39m.\u001b[39muint8)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ksw07/Desktop/%ED%8C%8C%EC%9D%B4%EB%84%90%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2022-Multi_PJT_3_Flask-master/2022-Multi_PJT_3_Flask/%EC%98%81%EC%83%81%20%ED%94%84%EB%A0%88%EC%9E%84%20%EB%A7%9E%EC%B6%94%EA%B8%B0_%EB%93%B1%EA%B8%89%EC%B6%94%EA%B0%80.ipynb#ch0000014?line=57'>58</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\opencv\\lib\\site-packages\\PIL\\ImageFont.py:959\u001b[0m, in \u001b[0;36mtruetype\u001b[1;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[39mreturn\u001b[39;00m FreeTypeFont(font, size, index, encoding, layout_engine)\n\u001b[0;32m    958\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 959\u001b[0m     \u001b[39mreturn\u001b[39;00m freetype(font)\n\u001b[0;32m    960\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_path(font):\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\opencv\\lib\\site-packages\\PIL\\ImageFont.py:956\u001b[0m, in \u001b[0;36mtruetype.<locals>.freetype\u001b[1;34m(font)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfreetype\u001b[39m(font):\n\u001b[1;32m--> 956\u001b[0m     \u001b[39mreturn\u001b[39;00m FreeTypeFont(font, size, index, encoding, layout_engine)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\opencv\\lib\\site-packages\\PIL\\ImageFont.py:247\u001b[0m, in \u001b[0;36mFreeTypeFont.__init__\u001b[1;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    245\u001b[0m                 load_from_bytes(f)\n\u001b[0;32m    246\u001b[0m             \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfont \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mgetfont(\n\u001b[0;32m    248\u001b[0m         font, size, index, encoding, layout_engine\u001b[39m=\u001b[39;49mlayout_engine\n\u001b[0;32m    249\u001b[0m     )\n\u001b[0;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     load_from_bytes(font)\n",
      "\u001b[1;31mOSError\u001b[0m: cannot open resource"
     ]
    }
   ],
   "source": [
    "# target + user 영상 넓이를 640으로 맞추기\n",
    "import imutils\n",
    "\n",
    "w = 640\n",
    "h = 720\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 1/0.03, (w, h))\n",
    "\n",
    "ss = []\n",
    "text = ''\n",
    "sc = []\n",
    "\n",
    "for num, i in enumerate(np.arange(0, du, 0.03)):\n",
    "   \n",
    "    img = video_clip.get_frame(i)\n",
    "    img2 = video_clip2.get_frame(i)\n",
    "    \n",
    "    img_taget = imutils.resize(img, width=640)\n",
    "    img_user = imutils.resize(img2, width=640)\n",
    "    \n",
    "    if (num+1) % 20 == 0:\n",
    "        ss_mean = round(np.mean([i*100 for i in ss[num-18:] if i is not None]))\n",
    "        sc.append(ss_mean)\n",
    "        score = ('BAD' if ss_mean <= 30 else 'MISS' if ss_mean <= 40 else 'GOOD' if ss_mean <= 50 else 'VERY GOOD' if ss_mean <= 60 else 'PERPECT')\n",
    "        text = f'{score}'\n",
    "\n",
    "    try:\n",
    "        result = detectPose(img, pose_video)[1]\n",
    "        result2 = detectPose(img2, pose_video)[1]\n",
    "        \n",
    "        similarity = get_sim(result, result2)\n",
    "        ss.append(similarity)\n",
    "\n",
    "        numpy_vertical = np.vstack((img_taget, img_user))\n",
    "        \n",
    "        ver_cv = cv2.cvtColor(numpy_vertical, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        cv2.putText(ver_cv,  text, (10, 380), cv2.FONT_HERSHEY_DUPLEX, 2, (199, 114, 255), 2, cv2.LINE_AA)        # BGR\n",
    "        cv2.imshow(\"target\", ver_cv)\n",
    "        \n",
    "    except:\n",
    "        numpy_vertical = np.vstack((img_taget, img_user))\n",
    "        ss.append(None)\n",
    "        \n",
    "        ver_cv = cv2.cvtColor(numpy_vertical, cv2.COLOR_BGR2RGB)\n",
    "        cv2.putText(ver_cv,  text, (10, 380), cv2.FONT_HERSHEY_DUPLEX, 2, (199, 114, 255), 2, cv2.LINE_AA)        # BGR\n",
    "        cv2.imshow(\"target\", ver_cv)\n",
    "\n",
    "    out.write(ver_cv) #프레임 쓰기\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# 점수 프레임 만들기\n",
    "font = ImageFont.truetype(\"C:/Users/min/AppData/Local/Microsoft/Windows/Fonts/NanumSquareOTF.otf\", 40)\n",
    "\n",
    "img = np.full(ver_cv.shape, (255, 255, 255), np.uint8)\n",
    "img = Image.fromarray(img)\n",
    "\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "total_score = round(np.mean(sc))\n",
    "text = f\"{total_score} 점\"\n",
    "text_score = ('FAIL' if total_score <= 30 else 'BAD' if total_score <= 40 else 'OK' if total_score <= 50 else 'GOOD' if total_score <= 60 else 'GREAT')\n",
    "\n",
    "draw.text((270, 100),  text, font=font, fill=(0, 0, 0))\n",
    "draw.text((270, 200),  text_score, font=font, fill=(0, 0, 0))\n",
    "\n",
    "img = np.array(img)\n",
    "\n",
    "cv2.imshow(\"target\", img)\n",
    "for i in range(0, 100):\n",
    "    out.write(img) #프레임 쓰기\n",
    "cv2.waitKey(5000)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 opencv로 재생되는지 확인\n",
    "\n",
    "w = 640\n",
    "h = 720\n",
    "cap = cv2.VideoCapture('output.avi')\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) # 또는 cap.get(3)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # 또는 cap.get(4)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX') # 코덱 정의\n",
    "out = cv2.VideoWriter('otter_out.avi', fourcc, fps, (int(width), int(height))) # VideoWriter 객체 정의\n",
    "\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"영상을 재생할수 없습니다.\")\n",
    "        break\n",
    "    frame = cv2.cvtColor(frame, cv2.COLORBGR2RGB)\n",
    "    cv2.imshow('output', frame)\n",
    "    if cv2.waitKey(42) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cv2.VideoWriter('0.avi', fourcc, 1/0.03, (w, h))\n",
    "\n",
    "# 점수 프레임 만들기\n",
    "total_score = round(np.mean(sc))\n",
    "text_score = ('F' if total_score <= 30 else 'D' if total_score <= 40 else 'C' if total_score <= 50 else 'B' if total_score <= 60 else 'A')\n",
    "score_img = cv2.imread(f'./score/{text_score}.PNG',1)\n",
    "score_img = imutils.resize(score_img, width=640)\n",
    "img_st = round((score_img.shape[0]-ver_cv.shape[0])/2)\n",
    "score_img = score_img[img_st:img_st+img_st + ver_cv.shape[0], :, :]\n",
    "\n",
    "for i in range(0, 100):\n",
    "    out.write(img) #프레임 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for i in sc:\n",
    "    k = 'FAIL' if i <= 30 else 'BAD' if i <= 40 else 'OK' if i <= 50 else 'GOOD' if i <= 60 else 'GREAT'\n",
    "    all.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 점수:  38\n",
      "BAD\n"
     ]
    }
   ],
   "source": [
    "total_score = round(np.mean(sc))\n",
    "print(\"최종 점수: \", total_score)\n",
    "print('FAIL' if total_score <= 30 else 'BAD' if total_score <= 40 else 'OK' if total_score <= 50 else 'GOOD' if total_score <= 60 else 'GREAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "videoclip = VideoFileClip(\"output.avi\")\n",
    "audioclip = AudioFileClip(\"target.mp3\")\n",
    "\n",
    "videoclip.audio = audioclip\n",
    "videoclip.write_videofile(\"new video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img_color = cv2.imread('./score/A.PNG',1) #cv2.IMREAD_COLOR\n",
    "img = imutils.resize(img_color, width=640)\n",
    "img = img[208:208+720, :]\n",
    "\n",
    "cv2.imshow('color', img)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bad Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i for i in ss if i is not None]\n",
    "start = np.arange(0, du, 0.03)[k.index(min(k))]\n",
    "end = min(start+20, du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_pose = VideoFileClip('./new video.mp4').subclip(start, end)\n",
    "bad_pose.write_videofile(\"./badpose2.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I-DLE - TOMBOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "video_clip = VideoFileClip('./영상/I-DLE TOMBOY.mp4').subclip(4.3, 83.48)\n",
    "video_clip2 = VideoFileClip('./영상/I-DLE TOMBOY_user.mp4').subclip(0.7, 74.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 싱크 맞는지 확인\n",
    "for i in np.arange(0, 84.2, 0.03):\n",
    "   \n",
    "    img = video_clip.get_frame(i)\n",
    "    img2 = video_clip2.get_frame(i)\n",
    "    \n",
    "    img_taget = cv2.resize(img, (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "    img_user = cv2.resize(img2, (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    numpy_vertical = np.vstack((img_taget, img_user))\n",
    "    numpy_vertical_concat = np.concatenate((img_taget, img_user), axis=0)\n",
    "    \n",
    "    ver_cv = cv2.cvtColor(numpy_vertical, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imshow(\"target\", ver_cv)\n",
    "        \n",
    "    # out.write(ver_cv) #프레임 쓰기\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹캠 녹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시간 변경 감지\n",
      "파일 생성: 2022-07-20 13시 45분.avi\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "cap = cv2.VideoCapture(cv2.CAP_DSHOW+0)\n",
    "cap.set(3, 720) # 윈도우 크기\n",
    "cap.set(4, 1080)\n",
    "fc = 30.0\n",
    "codec = cv2.VideoWriter_fourcc('D', 'I', 'V', 'X')\n",
    "count = 99\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    if count != time.strftime('%H',time.localtime(time.time())): # 시간이 바뀌면 영상파일을 새로 만든다. (시간으로 감지)\n",
    "        \n",
    "        count = time.strftime('%H',time.localtime(time.time()))\n",
    "        print('시간 변경 감지')\n",
    "        \n",
    "        out = cv2.VideoWriter(time.strftime('%Y-%m-%d %H시 %M분',time.localtime(time.time()))+'.avi', codec, fc, (int(cap.get(3)), int(cap.get(4))))\n",
    "        print('파일 생성:',time.strftime('%Y-%m-%d %H시 %M분',time.localtime(time.time()))+'.avi')\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    #frame = cv2.flip(frame,1) # 화면 반전 0: 상하, 1: 좌우\n",
    "    # 시간 텍스트 출력\n",
    "    cv2.putText(frame, text=time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), org=(30, 450), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0,255,0), thickness=2)\n",
    "    \n",
    "    if ret==True:\n",
    "        cv2.imshow('Record&Save', frame)\n",
    "        out.write(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps 30.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "# 이미지에 텍스트를 출력하는 함수\n",
    "def draw_text(img, text, x, y):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1\n",
    "    font_thickness = 2\n",
    "    text_color=(255, 0, 0)\n",
    "    text_color_bg=(0, 0, 0)\n",
    "\n",
    "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "    text_w, text_h = text_size\n",
    "    offset = 5\n",
    "\n",
    "    cv2.rectangle(img, (x - offset, y - offset), (x + text_w + offset, y + text_h + offset), text_color_bg, -1)\n",
    "    cv2.putText(img, text, (x, y + text_h + font_scale - 1), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "\n",
    "# 웹캠 연결\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# 웹캠에서 fps 값 획득\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print('fps', fps)\n",
    "\n",
    "if fps == 0.0:\n",
    "    fps = 30.0\n",
    "\n",
    "time_per_frame_video = 1/fps\n",
    "last_time = time.perf_counter()\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # 웹캠에서 이미지 읽어옴\n",
    "    ret,img_color = cap.read()\n",
    "\n",
    "    if ret == False:\n",
    "        print('웹캠에서 영상을 읽을 수 없습니다.')\n",
    "        break\n",
    "\n",
    "    # fsp 계산\n",
    "    time_per_frame = time.perf_counter() - last_time\n",
    "    time_sleep_frame = max(0, time_per_frame_video - time_per_frame)\n",
    "    time.sleep(time_sleep_frame)\n",
    "\n",
    "    real_fps = 1/(time.perf_counter()-last_time)\n",
    "    last_time = time.perf_counter()\n",
    "\n",
    "\n",
    "    x = 30\n",
    "    y = 50\n",
    "    text = '%.2f fps' % real_fps\n",
    "\n",
    "    # 이미지의 (x, y)에 텍스트 출력\n",
    "    draw_text(img_color, text, x, y)\n",
    "    cv2.imshow(\"Color\", img_color)\n",
    "\n",
    "\n",
    "    # ESC키 누르면 중지\n",
    "    if cv2.waitKey(1)&0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "# 노트북 웹캠에서 받아오는 영상을 저장하기\n",
    "\n",
    "# 기본 카메라 객체 생성\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 열렸는지 확인\n",
    "if not cap.isOpened():\n",
    "    print(\"Camera open failed!\")\n",
    "    sys.exit()\n",
    "\n",
    "# 웹캠의 속성 값을 받아오기\n",
    "# 정수 형태로 변환하기 위해 round\n",
    "w = round(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 카메라에 따라 값이 정상적, 비정상적\n",
    "\n",
    "if fps ==0 :\n",
    "    fps=30\n",
    "\n",
    "# fourcc 값 받아오기, *는 문자를 풀어쓰는 방식, *'DIVX' == 'D', 'I', 'V', 'X'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "\n",
    "# 1프레임과 다음 프레임 사이의 간격 설정\n",
    "delay = round(1000/fps)\n",
    "\n",
    "# 웹캠으로 찰영한 영상을 저장하기\n",
    "# cv2.VideoWriter 객체 생성, 기존에 받아온 속성값 입력\n",
    "out = cv2.VideoWriter('1.avi', fourcc, fps, (w, h))\n",
    "\n",
    "begins = []\n",
    "# 제대로 열렸는지 확인\n",
    "if not out.isOpened():\n",
    "    print('File open failed!')\n",
    "    cap.release()\n",
    "    sys.exit()\n",
    "    \n",
    "# 프레임을 받아와서 저장하기\n",
    "while True:                 # 무한 루프\n",
    "    ret, frame = cap.read() # 카메라의 ret, frame 값 받아오기\n",
    "\n",
    "    if not ret:             #ret이 False면 중지\n",
    "        break\n",
    "    \n",
    "    cv2.putText(frame, text=time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), org=(30, 450), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0,255,0), thickness=2)\n",
    "    cv2.imshow('frame', frame)\n",
    "    begins.append(time.time())\n",
    "    out.write(frame) # 영상 데이터만 저장. 소리는 X\n",
    "    \n",
    "    # print(cap.get(cv2.CAP_PROP_FPS))\n",
    "    if cv2.waitKey(delay) == 27: # esc를 누르면 강제 종료\n",
    "        end = time.time()\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카운트다운 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 0, -1):\n",
    "    zeros = np.zeros((720, 1280), dtype=np.uint8)\n",
    "    cv2.putText(zeros, str(i), (580, 360), cv2.FONT_HERSHEY_SIMPLEX, 10, (255,255,255), 20, cv2.LINE_AA)\n",
    "    cv2.imshow('test', zeros)\n",
    "    cv2.waitKey(1000)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 웹캠 연결\n",
    "cap = cv2.VideoCapture('./영상/I-DLE TOMBOY.mp4')\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # 웹캠에서 이미지 읽어옴\n",
    "    ret,img_color = cap.read()\n",
    "\n",
    "    if ret == False:\n",
    "        print('웹캠에서 영상을 읽을 수 없습니다.')\n",
    "        break\n",
    "    \n",
    "    # print(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    \n",
    "    if cap.get(cv2.CAP_PROP_POS_FRAMES) == 1:\n",
    "        for i in range(3, 0, -1):\n",
    "            zeros = np.zeros((720, 1280), dtype=np.uint8)\n",
    "            cv2.putText(zeros, str(i), (580, 360), cv2.FONT_HERSHEY_SIMPLEX, 10, (255,255,255), 20, cv2.LINE_AA)\n",
    "            cv2.imshow('test', zeros)\n",
    "            cv2.waitKey(1000)\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "    cv2.imshow(\"test\", img_color)\n",
    "\n",
    "\n",
    "    # ESC키 누르면 중지\n",
    "    if cv2.waitKey(1)&0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 2022-07-24-git-39a538f430-full_build-www.gyan.dev Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 12.1.0 (Rev2, Built by MSYS2 project)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-libsnappy --enable-zlib --enable-librist --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-libbluray --enable-libcaca --enable-sdl2 --enable-libdav1d --enable-libdavs2 --enable-libuavs3d --enable-libzvbi --enable-librav1e --enable-libsvtav1 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs2 --enable-libxvid --enable-libaom --enable-libjxl --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-frei0r --enable-libfreetype --enable-libfribidi --enable-liblensfun --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-ffnvcodec --enable-nvdec --enable-nvenc --enable-d3d11va --enable-dxva2 --enable-libmfx --enable-libshaderc --enable-vulkan --enable-libplacebo --enable-opencl --enable-libcdio --enable-libgme --enable-libmodplug --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libshine --enable-libtheora --enable-libtwolame --enable-libvo-amrwbenc --enable-libilbc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-ladspa --enable-libbs2b --enable-libflite --enable-libmysofa --enable-librubberband --enable-libsoxr --enable-chromaprint\n",
      "  libavutil      57. 30.100 / 57. 30.100\n",
      "  libavcodec     59. 39.100 / 59. 39.100\n",
      "  libavformat    59. 29.100 / 59. 29.100\n",
      "  libavdevice    59.  8.101 / 59.  8.101\n",
      "  libavfilter     8. 46.100 /  8. 46.100\n",
      "  libswscale      6.  8.101 /  6.  8.101\n",
      "  libswresample   4.  8.100 /  4.  8.100\n",
      "  libpostproc    56.  7.100 / 56.  7.100\n",
      "Input #0, matroska,webm, from 'raw_video.avi':\n",
      "  Metadata:\n",
      "    encoder         : Chrome\n",
      "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0(eng): Video: h264 (Constrained Baseline), yuv420p(progressive), 640x480, SAR 1:1 DAR 4:3, 1k tbr, 1k tbn (default)\n",
      "Codec AVOption b (set bitrate (in bits/s)) specified for output file #0 (test.mp4) has not been used for any stream. The most likely reason is either wrong type (e.g. a video option with no video streams) or that it is a private option of some encoder which was not actually used for any stream.\n",
      "Output #0, mp4, to 'test.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf59.29.100\n",
      "  Stream #0:0(eng): Video: h264 (Constrained Baseline) (avc1 / 0x31637661), yuv420p(progressive), 640x480 [SAR 1:1 DAR 4:3], q=2-31, 1k tbr, 16k tbn (default)\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "frame=    1 fps=0.0 q=-1.0 size=       0kB time=00:00:00.00 bitrate=N/A speed=N/A    \n",
      "frame=    2 fps=0.0 q=-1.0 size=       0kB time=00:00:00.05 bitrate=   7.7kbits/s speed=0.0674x    \n",
      "frame=   83 fps= 68 q=-1.0 Lsize=     274kB time=00:00:02.73 bitrate= 821.3kbits/s speed=2.24x    \n",
      "video:273kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.638373%\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -i raw_video.avi -c:a aac -b:a 192k -c:v copy test.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "video_clip = VideoFileClip('./test.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "video_clip = VideoFileClip('./영상/BTS Dynamite_target.mp4')\n",
    "du = video_clip.duration\n",
    "video_clip2 = VideoFileClip('./영상/BTS Dynamite_user.mp4').subclip(0.2, du+0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#합성시킬 두 개의 영상 열기\n",
    "cap1 = cv2.VideoCapture('./영상/BTS Dynamite_user.mp4')\n",
    "cap2 = cv2.VideoCapture('./영상/BTS Dynamite_target.mp4')\n",
    "\n",
    "if not cap1.isOpened() or not cap2.isOpened():\n",
    "\tsys.exit()\n",
    "    \n",
    "    \n",
    "#각 영상 프레임 수\n",
    "frame_cnt1 = round(cap1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_cnt2 = round(cap2.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = cap1.get(cv2.CAP_PROP_FPS)\n",
    "effect_frames = int(fps*2)\n",
    "\n",
    "delay = int(1000/fps)\n",
    "\n",
    "#영상 가로 세로 설정\n",
    "w = round(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = round(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "#비디오 코덱 설정\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "\n",
    "# out = cv2.VideoWriter('output.avi', fourcc, fps, (w,h))\n",
    "\n",
    "#1번 영상 열기\n",
    "for i in range(frame_cnt1 - effect_frames):\n",
    "    ret1, frame1 = cap1.read()\n",
    "    \n",
    "    if not ret1:\n",
    "        break\n",
    "        \n",
    "    # out.write(frame1)\n",
    "    cv2.imshow('frame',frame1)\n",
    "    cv2.waitKey(delay)\n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "#합성하기\n",
    "for i in range(effect_frames):\n",
    "    ret1, frame1 = cap1.read()\n",
    "    ret2, frame2 = cap2.read()\n",
    "    \n",
    "    #가중치 계산\n",
    "    #만약 i가 1인경우 alpha는 1- 1/48 , 1-alpha는 0에 가까운 값\n",
    "    alpha = 1.0 - i / effect_frames\n",
    "    frame = cv2.addWeighted(frame1, alpha, frame2, 1-alpha,0)\n",
    "    # out.write(frame)\n",
    "    \n",
    "for i in range(effect_frames, frame_cnt2):\n",
    "    ret2, frame2 = cap2.read()\n",
    "    \n",
    "    if not ret2:\n",
    "        break\n",
    "        \n",
    "    # out.write(frame2)\n",
    "    cv2.imshow('frame', frame2)\n",
    "    cv2.waitKey(delay)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.6.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('opencv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77e823099e91b2f301124c808b67c4132b7332bf63fbdf77b0f894f48999cafa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
